#!/usr/bin/env python3
"""
Script to clean up duplicate indexes in the chatbot system
"""
import os
import json
import faiss
from per_user_index import _user_paths, _create_new_index

def clean_user_index(user_id):
    """Clean duplicate entries from user's index"""
    idx_path, meta_path = _user_paths(user_id)
    
    if not os.path.exists(idx_path) or not os.path.exists(meta_path):
        print(f"No index found for user {user_id}")
        return
    
    # Load existing index and metadata
    idx = faiss.read_index(idx_path)
    with open(meta_path, 'r') as f:
        meta = json.load(f)
    
    print(f"Original index size: {idx.ntotal}")
    print(f"Original metadata items: {len(meta['items'])}")
    
    # Find unique paths
    unique_paths = {}
    duplicate_ids = []
    
    for item_id, item_data in meta['items'].items():
        path = item_data['path']
        if path in unique_paths:
            # This is a duplicate
            duplicate_ids.append(int(item_id))
            print(f"Found duplicate: ID {item_id} -> {path}")
        else:
            unique_paths[path] = {
                'id': int(item_id),
                'data': item_data
            }
    
    if not duplicate_ids:
        print("No duplicates found!")
        return
    
    print(f"Found {len(duplicate_ids)} duplicate entries")
    
    # Create new index with only unique entries
    if unique_paths:
        # Get the dimension from the first vector
        first_item = next(iter(unique_paths.values()))
        # We need to get the actual vector dimension, let's use a default
        new_idx = _create_new_index(512)  # CLIP ViT-Large has 768 dimensions, but let's be safe
        
        # Rebuild index with unique entries only
        new_meta = {"_next_id": 1, "items": {}}
        
        for path, item_info in unique_paths.items():
            # We can't easily reconstruct the vectors, so we'll need to re-embed
            # For now, let's just clean the metadata and let the user re-index
            new_meta["items"][str(new_meta["_next_id"])] = item_info['data']
            new_meta["_next_id"] += 1
        
        # Save cleaned metadata
        with open(meta_path, 'w') as f:
            json.dump(new_meta, f)
        
        print(f"Cleaned metadata. {len(unique_paths)} unique items remaining.")
        print("Note: You may need to re-index images to rebuild the FAISS index.")
    else:
        print("No unique items found!")

def main():
    import sys
    if len(sys.argv) != 2:
        print("Usage: python clean_duplicate_indexes.py <username>")
        sys.exit(1)
    
    username = sys.argv[1]
    print(f"Cleaning duplicate indexes for user: {username}")
    clean_user_index(username)

if __name__ == "__main__":
    main()
